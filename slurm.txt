#!/bin/bash

#SBATCH --nodes=1 # request one node

#SBATCH --gres=gpu:1 #If you just need one gpu, you're done, if you need more you can change the number

#SBATCH --partition=gpu #coms-instruction #specify the gpu partition

#SBATCH --cpus-per-task=8  # ask for 8 cpus

##SBATCH --mem=128# Maximum amount of memory this job will be given, try to estimate this to the best of your ability. This asks for 128 GB of ram.

#SBATCH --time=3-00:00:00 # this set up the days that the script is going to run

# everything below this line is optional, but are nice to have quality of life things

#SBATCH --output=%J.out # tell it to store the output console text to a file called job.<assigned job number>.out

#SBATCH --error=%J.err # tell it to store the error messages from the program (if it doesn't write them to normal console output) to a file called job.<assigned job muber>.err

#SBATCH --job-name="SS" # a nice readable name to give your job so you know what it is when you see it in the queue, instead of just numbers

# #SBATCH --mail-user=janselh@iastate.edu   # email address
# #SBATCH --mail-type=BEGIN
# #SBATCH --mail-type=END
# #SBATCH --mail-type=FAIL

# under this we just do what we would normally do to run the program, everything above this line is used by slurm to tell it what your job needs for resources

# let's load the modules we need to do what we're going to do

module purge  # so that runs are always identical
module list

module load ml-gpu
module list

# ml-gpu pip3 install tfds-nightly
# ml-gpu pip3 install tensorflow-gan --user
# ml-gpu pip3 install imutils

#ml-gpu pip3 install tensorflow-hub 

# let's make sure we're where we expect to be in the filesystem tree
cd /work/LAS/jannesar-lab/SS_Clustering/simclr


# make all possible flags variables
logtostderr='default'
alsologtostderr='default'
log_dir='default'
v='default'
verbosity='default'
logger_levels='default'
stderrthreshold='default'
showprefixforinfo='default'
run_with_pdb='default'
pdb_post_mortem='default'
pdb='default'
run_with_profiling='default'
profile_file='default'
use_cprofile_for_profiling='default'
only_check_args='default'
op_conversion_fallback_to_while_loop='default'
test_random_seed='default'
test_srcdir='default'
test_tmpdir='default'
test_randomize_ordering_seed='default'
xml_output_file='default'
tfhub_cache_dir='default'
lr_slope_warm_epochs='default'
attention_output='default'
model_using='default'
learning_rate='default'
learning_rate_scaling='default'
warmup_epochs='default'
weight_decay='default'
batch_norm_decay='default'
train_batch_size='default'
train_split='default'
train_epochs='default'
train_steps='default'
eval_batch_size='default'
train_summary_steps='default'
checkpoint_epochs='default'
checkpoint_steps='default'
eval_split='default'
dataset='default'
cache_dataset='default'
mode='default'
train_mode='default'
checkpoint='default'
variable_schema='?!global_step'
zero_init_logits_layer=False
master=None
model_dir='default'
data_dir='default'
use_tpu=False
tpu_name=None
tpu_zone=None
gcp_project=None


# set all flags we might change as their default values
# all of these flags are in run command, so they need a value besides 'default'
lr_slope_warm_epochs=10
attention_output='hard'
model_using='simclr'
learning_rate=0.3,
learning_rate_scaling='linear'
warmup_epochs=10
weight_decay=1e-4
batch_norm_decay=0.9
train_batch_size=512
train_split='train'
train_epochs=100
train_steps=0
eval_batch_size=256
train_summary_steps=100
checkpoint_epochs=1
checkpoint_steps=0
eval_split='validation'
dataset='imagenet2012'
cache_dataset=False
train_mode='pretrain'
checkpoint=None
fine_tune_after_block=-1
use_tpu=False
optimizer='lars'
momentum=0.9
keep_checkpoint_max=10
keep_hub_module_max=1
temperature=0.1
hidden_norm=True
proj_head_mode='nonlinear'
proj_out_dim=128
num_proj_layers=3
num_attn_layers=2
ft_proj_selector=0
global_bn=True
width_multiplier=1
resnet_depth=50
sk_ratio=0.0
se_ratio=0.0
image_size=224
crop_size=28
color_jitter_strength=1.0
use_blur=True

model_dir=None
data_dir=None
checkpoint=None  # checkpoint needs to be included in run if changing from None (uncomment at bottom)

# =====================================================================================================================
# area to modify each run below
# =====================================================================================================================

# set relevant flags that we do not want to be default values


# RUN 1 --------------------------------------------------------------------
# pretrain on imagenet
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run1
mode='train'
train_mode='pretrain'
train_batch_size=32
train_epochs=10
resnet_depth=50
learning_rate=1.0
data_dir=/work/LAS/jannesar-lab/janselh/data/


dataset=imagenet2012
image_size=224
crop_size=32

# some flags given by simclr readme instructions
temperature=0.5
use_tpu=False

# no need to use blur or color jitter for only 10 epochs
use_blur=False
color_jitter_strength=0

# flags to play around with
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=10
attention_output='hard'

#Observations:
#The first 30k steps the model did not improve at all and the contrastive
#accuracy was 0.031.  After this, the contrastive loss and accuracy started
#fluctuating but never improved.


# RUN 2 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run2
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=20  # make slope less steep
attention_output='hard'

#Observations:
#The first 60k steps the model did not improve at all and the contrastive
#accuracy was 0.031.  After this, the contrastive loss and accuracy started
#fluctuating but never improved.


# RUN 3 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run3
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=10
attention_output='softmax'

#Observations:
#This model did very bad from the beginning, the accuracy is low and the loss is
#high.

# RUN 4 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run4
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=20
attention_output='softmax'

#Observations:
#This model did very bad from the beginning, the accuracy is low and the loss is
#high.

# RUN 5 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run5
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=10
attention_output='L2'

#Observations:
#The model performed bad from the beginning but something strange happened is
#that after 200k steps the model results were constant.

# RUN 6 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run6
model_using='attn_simclr'
optimizer=adam
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=20
attention_output='L2'

#Observations:
#Similar to 5 but instead of being constant it stayed on the same level with some noise.

# repeat above 6 with lars optimizer
# RUN 7 ===========================================================================================
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run7
model_using='attn_simclr'
optimizer=lars
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=10
attention_output='hard'

#Observations:
#Performed good at the beginning and after 30k convergence was messed up.

# RUN 8 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run8
model_using='attn_simclr'
optimizer=lars
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=20  # make slope less steep
attention_output='hard'

#Observations:
#Performed good at the beginning and after 40k convergence was messed up.

# RUN 9 --------------------------------------------------------------------
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run9
model_using='attn_simclr'
optimizer=lars
learning_rate=1.0
warmup_epochs=3
lr_slope_warm_epochs=10
attention_output='softmax'

#Observations:
#One of the best performances, it did very good at the beginning and then it
#started decreasing but with a low rate. Not like other runs.

## RUN 10 --------------------------------------------------------------------
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run10
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=3
#lr_slope_warm_epochs=20
#attention_output='softmax'

#Observations:
#Very similar to run 9 just a little below in performance.

## RUN 11 --------------------------------------------------------------------
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run11
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=3
#lr_slope_warm_epochs=10
#attention_output='L2'


#Observations:
#Good at the beginning, after 12k steps it messed up.

## RUN 12 --------------------------------------------------------------------
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run12
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=3
#lr_slope_warm_epochs=20
#attention_output='L2'

#Observations:
#Bad performance overall, good until 10k.

# RUN 13 --------------------------------------------------------------------
# same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run13
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.000001
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'
#
#
## RUN 14 --------------------------------------------------------------------
## same as model 9 tested with the old learning rate scheduler
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run14
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=3
#lr_slope_warm_epochs=10
#attention_output='softmax'
#
##Observations:
##Very similar to run 9, I was testing the old scheduler.
#
# RUN 15 --------------------------------------------------------------------
# same as model 9 tested with the old learning rate scheduler
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run15
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'
#
#
## RUN 16 --------------------------------------------------------------------
## same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run16
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=1.0
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'

# RUN 17 --------------------------------------------------------------------
# same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run17
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.001
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'
#
## RUN 18 --------------------------------------------------------------------
## same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run18
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=0.001
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'

# RUN 19------------------------------------------------------------------
# same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run19
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.0001
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'

# RUN 20------------------------------------------------------------------
# same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run20
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=0.0001
#warmup_epochs=0
#lr_slope_warm_epochs=20
#attention_output='softmax'

## RUN 21------------------------------------------------------------------
## same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run21
#model_using='attn_simclr'
#optimizer=lars
#learning_rate=0.0001
#warmup_epochs=0
#lr_slope_warm_epochs=10
#attention_output='softmax'

# RUN 22------------------------------------------------------------------
## same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run22
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.0001
#warmup_epochs=0
#lr_slope_warm_epochs=10
#attention_output='softmax'

# RUN 23------------------------------------------------------------------
## same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run23
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.00001
#warmup_epochs=0
#lr_slope_warm_epochs=10
#attention_output='softmax'

# RUN 24------------------------------------------------------------------
# same as model 9 without warmup 
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run24
#model_using='attn_simclr'
#optimizer=adam
#learning_rate=0.000001
#warmup_epochs=0
#lr_slope_warm_epochs=10
#attention_output='softmax'

## SIMCLR--------------------------------------------------------------------
# RUN 1 --------------------------------------------------------------------
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/simclr_run1
#model_using='simclr'
#optimizer=lars
#learning_rate=1.0
#warmup_epochs=3
#lr_slope_warm_epochs=10
#attention_output='L2'
#
## RUN 2 --------------------------------------------------------------------
#model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/simclr_run2
#model_using='simclr'
#optimizer=adam
#learning_rate=1.0
#warmup_epochs=0
#lr_slope_warm_epochs=10
#attention_output='L2'

# RUN Debug --------------------------------------------------------------------
mode='eval'
train_mode='finetune'
fine_tune_after_block=4
zero_init_logits_layer=True
variable_schema="(?!global_step|(?:.*/|^)Momentum|head)" 
global_bn=False
dataset=oxford_iiit_pet
#dataset=cifar100
#dataset=cifar10
#dataset=imagenet2012
checkpoint=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run19/
model_dir=/work/LAS/jannesar-lab/janselh/SS_Clustering/simclr/attn_boards/attn_run19/$dataset
model_using='attn_simclr'
weight_decay=0.0 
optimizer=momentum
eval_split='test'
#eval_split='validation'
learning_rate=0.1
warmup_epochs=0
lr_slope_warm_epochs=10
attention_output='L2'



# =====================================================================================================================
# area to modify each run above
# =====================================================================================================================



# echo all possible flags
echo 'logtostderr = ' $logtostderr
echo 'alsologtostderr = ' $alsologtostderr
echo 'log_dir = ' $log_dir
echo 'v = ' $v
echo 'verbosity = ' $verbosity
echo 'logger_levels = ' $logger_levels
echo 'stderrthreshold = ' $stderrthreshold
echo 'showprefixforinfo = ' $showprefixforinfo
echo 'run_with_pdb = ' $run_with_pdb
echo 'pdb_post_mortem = ' $pdb_post_mortem
echo 'pdb = ' $pdb
echo 'run_with_profiling = ' $run_with_profiling
echo 'profile_file = ' $profile_file
echo 'use_cprofile_for_profiling = ' $use_cprofile_for_profiling
echo 'only_check_args = ' $only_check_args
echo 'op_conversion_fallback_to_while_loop = ' $op_conversion_fallback_to_while_loop
echo 'test_random_seed = ' $test_random_seed
echo 'test_srcdir = ' $test_srcdir
echo 'test_tmpdir = ' $test_tmpdir
echo 'test_randomize_ordering_seed = ' $test_randomize_ordering_seed
echo 'xml_output_file = ' $xml_output_file
echo 'tfhub_cache_dir = ' $tfhub_cache_dir
echo 'lr_slope_warm_epochs = ' $lr_slope_warm_epochs
echo 'attention_output = ' $attention_output
echo 'model_using = ' $model_using
echo 'learning_rate = ' $learning_rate
echo 'learning_rate_scaling = ' $learning_rate_scaling
echo 'warmup_epochs = ' $warmup_epochs
echo 'weight_decay = ' $weight_decay
echo 'batch_norm_decay = ' $batch_norm_decay
echo 'train_batch_size = ' $train_batch_size
echo 'train_split = ' $train_split
echo 'train_epochs = ' $train_epochs
echo 'train_steps = ' $train_steps
echo 'eval_batch_size = ' $eval_batch_size
echo 'train_summary_steps = ' $train_summary_steps
echo 'checkpoint_epochs = ' $checkpoint_epochs
echo 'checkpoint_steps = ' $checkpoint_steps
echo 'eval_split = ' $eval_split
echo 'dataset = ' $dataset
echo 'cache_dataset = ' $cache_dataset
echo 'mode = ' $mode
echo 'train_mode = ' $train_mode
echo 'checkpoint = ' $checkpoint
echo 'variable_schema = ' $variable_schema
echo 'zero_init_logits_layer = ' $zero_init_logits_layer
echo 'fine_tune_after_block = ' $fine_tune_after_block
echo 'master = ' $master
echo 'model_dir = ' $model_dir
echo 'data_dir = ' $data_dir
echo 'use_tpu = ' $use_tpu
echo 'tpu_name = ' $tpu_name
echo 'tpu_zone = ' $tpu_zone
echo 'gcp_project = ' $gcp_project
echo 'optimizer = ' $optimizer
echo 'momentum = ' $momentum
echo 'eval_name = ' $eval_name
echo 'keep_checkpoint_max = ' $keep_checkpoint_max
echo 'keep_hub_module_max = ' $keep_hub_module_max
echo 'temperature = ' $temperature
echo 'hidden_norm = ' $hidden_norm
echo 'proj_head_mode = ' $proj_head_mode
echo 'proj_out_dim = ' $proj_out_dim
echo 'num_proj_layers = ' $num_proj_layers
echo 'num_attn_layers = ' $num_attn_layers
echo 'ft_proj_selector = ' $ft_proj_selector
echo 'global_bn = ' $global_bn
echo 'width_multiplier = ' $width_multiplier
echo 'resnet_depth = ' $resnet_depth
echo 'sk_ratio = ' $sk_ratio
echo 'se_ratio = ' $se_ratio
echo 'image_size = ' $image_size
echo 'crop_size = ' $crop_size
echo 'color_jitter_strength = ' $color_jitter_strength
echo 'use_blur = ' $use_blur


# run model with all possible flags fed in that we don't want as default
echo 'run.py'
ml-gpu python3 run.py \
  --lr_slope_warm_epochs=$lr_slope_warm_epochs \
  --attention_output=$attention_output \
  --model_using=$model_using \
  --learning_rate=$learning_rate \
  --learning_rate_scaling=$learning_rate_scaling \
  --warmup_epochs=$warmup_epochs \
  --weight_decay=$weight_decay \
  --batch_norm_decay=$batch_norm_decay \
  --train_batch_size=$train_batch_size \
  --train_split=$train_split \
  --train_epochs=$train_epochs \
  --train_steps=$train_steps \
  --eval_batch_size=$eval_batch_size \
  --train_summary_steps=$train_summary_steps \
  --checkpoint_epochs=$checkpoint_epochs \
  --checkpoint_steps=$checkpoint_steps \
  --eval_split=$eval_split \
  --dataset=$dataset \
  --cache_dataset=$cache_dataset \
  --mode=$mode \
  --train_mode=$train_mode \
  --fine_tune_after_block=$fine_tune_after_block \
  --model_dir=$model_dir \
  --data_dir=$data_dir \
  --use_tpu=$use_tpu \
  --tpu_zone=$tpu_zone \
  --optimizer=$optimizer \
  --momentum=$momentum \
  --keep_checkpoint_max=$keep_checkpoint_max \
  --keep_hub_module_max=$keep_hub_module_max \
  --temperature=$temperature \
  --hidden_norm=$hidden_norm \
  --proj_head_mode=$proj_head_mode \
  --proj_out_dim=$proj_out_dim \
  --num_proj_layers=$num_proj_layers \
  --num_attn_layers=$num_attn_layers \
  --ft_proj_selector=$ft_proj_selector \
  --global_bn=$global_bn \
  --width_multiplier=$width_multiplier \
  --resnet_depth=$resnet_depth \
  --sk_ratio=$sk_ratio \
  --se_ratio=$se_ratio \
  --image_size=$image_size \
  --crop_size=$crop_size \
  --color_jitter_strength=$color_jitter_strength \
  --use_blur=$use_blur \
#  --checkpoint=$checkpoint \

echo 'finished run.py'

